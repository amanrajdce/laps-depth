apiVersion: v1
kind: Pod # Deployment will automatically restart when killed. Use Pod if not needed
metadata:
  labels:
    k8s-app: research
  generateName: amraj-pod-test- # replace <your_name> with something that identifies you
  namespace: ecewcsng

spec:
  containers:
  - name: research
    image: amanraj42/tensorflow-gpu-ray:v1.11
    imagePullPolicy: Always
    workingDir: /ceph/amanraj/codes/pba-signet
    resources:
      requests:
        memory: "8Gi"        # requests 8Gi of memory minimum
        cpu: "2"             # requests 2 CPU
        #nvidia.com/gpu: 1
        ephemeral-storage: 70Gi
      limits:
        memory: "8Gi"
        cpu: "2"
        #nvidia.com/gpu: 1 # requesting X GPU
        ephemeral-storage: 70Gi
    volumeMounts:
    - mountPath: /dev/shm
      name: dshm
    - mountPath: /ceph
      name: ceph
    - mountPath: /mnt/data
      name: data
  initContainers:
    - name: init-data
      image: gitlab-registry.nautilus.optiputer.net/prp/gsutil
      imagePullPolicy: Always
      args:
        - gsutil
        - "-m"
        - rsync
        - "-erP"
        - /ceph/data/nvme_data_sync_tar/
        - /mnt/dest/
      volumeMounts:
        - name: ceph
          mountPath: /ceph
        - name: data
          mountPath: /mnt/dest
    - name: extract-data      # run data extraction scripts
      image: gitlab-registry.nautilus.optiputer.net/prp/gsutil
      workingDir: /mnt/data
      command: ["/bin/sh"]
      args: ["extract_tarball.sh"]
      resources:
        requests:
          memory: "8Gi"         # requests memory and CPU to speed up extraction
          cpu: "2"              # requests 2 CPU
        limits:
          memory: "8Gi"
          cpu: "2"
      volumeMounts:
        - name: data
          mountPath: /mnt/data
  volumes:
    - name: data
      emptyDir: {}
    - name: dshm
      emptyDir:
        medium: Memory
    - name: ceph
      flexVolume:
        driver: ceph.rook.io/rook
        fsType: ceph
        options:
          clusterNamespace: rook
          fsName: nautilusfs
          path: /ecewcsng
          mountUser: ecewcsng
          mountSecret: ceph-fs-secret
  affinity:
    #nodeAffinity:
    #  requiredDuringSchedulingIgnoredDuringExecution:
    #    nodeSelectorTerms:
    #    - matchExpressions:
    #      - key: gpu-type
    #        operator: In # Use NotIn for other types
    #        values:
    #        - 1080Ti
  nodeSelector:
    kubernetes.io/hostname: clu-fiona2.ucmerced.edu
    nautilus.io/disktype: nvme
  tolerations:                                      # use nodes that can't access public internet
    - key: "nautilus.io/science-dmz"
      operator: "Exists"
      effect: "NoSchedule"
    - key: "nautilus.io/bharadia"                   # as a priority , bharadia
      operator: "Exists"
      effect: "NoSchedule"

#kubernetes.io/hostname: clu-fiona2.ucmerced.edu # this node has 8 - 1080-Ti but access to public internet