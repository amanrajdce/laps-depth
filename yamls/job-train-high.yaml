apiVersion: batch/v1
kind: Job # Deployment will automatically restart when killed. Use Pod if not needed
metadata:
  labels:
    k8s-app: research
  generateName: amraj-train-high-m2-p1-25q- # replace <your_name> with something that identifies you
  namespace: ecewcsng
spec:
  template:
    metadata:
      labels:
        k8s-app: research
    spec:
      restartPolicy: Never
      containers:
      - name: research
        image: amanraj42/tensorflow-gpu-ray:v1.11
        imagePullPolicy: Always
        workingDir: /ceph/amanraj/codes/pba-signet
        command: ["/bin/sh"]                     # replace this with your own job execution scripts
        args: ["pod/train_mono2_rand_single.sh", "pod", "00001"]
        resources:
          requests:
            memory: "15Gi"          # requests 15Gi of memory minimum
            cpu: "4"                # requests 2 CPU extras
            nvidia.com/gpu: 1       # requesting X GPU
            ephemeral-storage: 80Gi
          limits:
            memory: "55Gi"
            cpu: "6"
            nvidia.com/gpu: 1
            ephemeral-storage: 80Gi
        volumeMounts:
        - mountPath: /dev/shm
          name: dshm
        - mountPath: /ceph
          name: ceph
        - mountPath: /mnt/data
          name: data
      initContainers:         # run first init container for copying data to nvme drive
        - name: init-data
          image: gitlab-registry.nautilus.optiputer.net/prp/gsutil
          args:
            - gsutil
            - "-m"
            - rsync
            - "-erP"
            - /ceph/data/nvme_data_sync_tar/
            - /mnt/dest/
          volumeMounts:
            - name: ceph
              mountPath: /ceph
            - name: data
              mountPath: /mnt/dest
        - name: extract-data      # run data extraction scripts
          image: gitlab-registry.nautilus.optiputer.net/prp/gsutil
          workingDir: /mnt/data
          command: ["/bin/sh"]
          args: ["extract_tarball.sh"]
          resources:
            requests:
              memory: "8Gi"         # requests memory and CPU to speed up extraction
              cpu: "2"              # requests 2 CPU
            limits:
              memory: "8Gi"
              cpu: "2"
          volumeMounts:
            - name: data
              mountPath: /mnt/data
      volumes:
        - name: data
          emptyDir: {}
        - name: dshm
          emptyDir:
            medium: Memory
        - name: ceph
          flexVolume:
            driver: ceph.rook.io/rook
            fsType: ceph
            options:
              clusterNamespace: rook
              fsName: nautilusfs
              path: /ecewcsng
              mountUser: ecewcsng
              mountSecret: ceph-fs-secret
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: gpu-type
                operator: In # Use NotIn for other types
                values:
                - 1080Ti
                #- 2080Ti
      nodeSelector:
        #kubernetes.io/hostname: clu-fiona2.ucmerced.edu
        nautilus.io/disktype: nvme
      tolerations:                                      # can use nodes that can't access public internet
        #- key: "nautilus.io/science-dmz"
        #  operator: "Exists"
        #  effect: "NoSchedule"
        #- key: "nautilus.io/bharadia"                   # put as a priority , ecewcsng nodes
        #  operator: "Exists"
        #  effect: "NoSchedule"
        - key: "nautilus.io/amraj"                      # use the assigned node to amraj
          operator: "Exists"
          effect: "NoSchedule"

#kubernetes.io/hostname: clu-fiona2.ucmerced.edu, this node has 8 - 1080-Ti but no access to public internet